{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e97bafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import shapiro, levene, f_oneway, kruskal\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "import scikit_posthocs as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74ff100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to the parsefile.py, we parse the results files:\n",
    "\n",
    "def parse_file(file_path):\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    blocks = re.split(r\"Performance counter stats for 'system wide':\", content)\n",
    "    rows = []\n",
    "\n",
    "    def clean_float(s):\n",
    "        return float(s.replace('\\u202F', '').replace(' ', '').replace(',', '.'))\n",
    "\n",
    "    def clean_int(s):\n",
    "        return int(s.replace('\\u202F', '').replace(' ', ''))\n",
    "\n",
    "    for block in blocks[1:]:\n",
    "        pkg_match = re.search(r\"([\\d,]+) Joules power/energy-pkg/\", block)\n",
    "        core_match = re.search(r\"([\\d,]+) Joules power/energy-cores/\", block)\n",
    "        cycles_match = re.search(r\"([\\d\\s\\u202F]+) +cycles\", block)\n",
    "        instr_match = re.search(r\"([\\d\\s\\u202F]+) +instructions\", block)\n",
    "        cache_ref_match = re.search(r\"([\\d\\s\\u202F]+) +cache-references\", block)\n",
    "        cache_miss_match = re.search(r\"([\\d\\s\\u202F]+) +cache-misses\", block)\n",
    "        cs_match = re.search(r\"([\\d\\s\\u202F]+) +cs\", block)\n",
    "        migrations_match = re.search(r\"([\\d\\s\\u202F]+) +migrations\", block)\n",
    "        pf_match = re.search(r\"([\\d\\s\\u202F]+) +page-faults\", block)\n",
    "\n",
    "        data = {\n",
    "            \"energy_pkg\": clean_float(pkg_match.group(1)),\n",
    "            \"energy_cores\": clean_float(core_match.group(1)),\n",
    "            \"cycles\": clean_int(cycles_match.group(1)),\n",
    "            \"instructions\": clean_int(instr_match.group(1)),\n",
    "            \"cache_references\": clean_int(cache_ref_match.group(1)),\n",
    "            \"cache_misses\": clean_int(cache_miss_match.group(1)),\n",
    "            \"cs\": clean_int(cs_match.group(1)),\n",
    "            \"migrations\": clean_int(migrations_match.group(1)),\n",
    "            \"page_faults\": clean_int(pf_match.group(1)),\n",
    "        }\n",
    "        rows.append(data)\n",
    "\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70037f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   energy_pkg  energy_cores      cycles  instructions  cache_references  \\\n",
      "0       42.26          4.75  2173328453     775201095          91714222   \n",
      "1       53.29          5.25  2336570407     885062192          95272255   \n",
      "2       46.18          5.14  2194307013     791346536          85571384   \n",
      "3       42.57          5.08  2294232196     828141032          95937502   \n",
      "4       44.17          4.88  2144127984     785100053          84155930   \n",
      "5       46.70          4.95  2288881461     857928383          92242934   \n",
      "6       52.30          4.84  2291043947     756573771          90915013   \n",
      "7       41.00          4.98  2226272619     789312490          92898751   \n",
      "8       47.99          5.12  2404529453     791234031          97021764   \n",
      "9       45.71          4.90  2231443685     798998783          92434134   \n",
      "\n",
      "   cache_misses     cs  migrations  page_faults                          test  \\\n",
      "0      39074097  29519          91          172  test_leo_skeleton_base_empty   \n",
      "1      39832935  29968         136         1180  test_leo_skeleton_base_empty   \n",
      "2      38768705  29267          69          188  test_leo_skeleton_base_empty   \n",
      "3      39848585  29458         101          879  test_leo_skeleton_base_empty   \n",
      "4      38120485  29052         109          166  test_leo_skeleton_base_empty   \n",
      "5      39699857  29552         104         1060  test_leo_skeleton_base_empty   \n",
      "6      39628657  29028         153          172  test_leo_skeleton_base_empty   \n",
      "7      38976677  29599          94          211  test_leo_skeleton_base_empty   \n",
      "8      39911823  28798          84         1166  test_leo_skeleton_base_empty   \n",
      "9      38798187  29413         116          172  test_leo_skeleton_base_empty   \n",
      "\n",
      "   run  \n",
      "0    1  \n",
      "1    2  \n",
      "2    3  \n",
      "3    4  \n",
      "4    5  \n",
      "5    6  \n",
      "6    7  \n",
      "7    8  \n",
      "8    9  \n",
      "9   10  \n"
     ]
    }
   ],
   "source": [
    "all_rows = []\n",
    "\n",
    "# Create dataframe \n",
    "\n",
    "file_names = [\n",
    "    \"test_leo_skeleton_base_empty.txt\",\n",
    "    \"test_leo_skeleton_base_long.txt\",\n",
    "    \"test_leo_skeleton_base_medium.txt\",\n",
    "    \"test_leo_skeleton_change_empty.txt\",\n",
    "    \"test_leo_skeleton_change_long.txt\", \n",
    "    \"test_leo_skeleton_change_medium.txt\", \n",
    "    \"test_leo_skeleton_change+logging_long.txt\",\n",
    "    \"test_leo_skeleton_change+logging_medium.txt\",\n",
    "    \"test_mu_skeleton_base_empty.txt\",\n",
    "    \"test_mu_skeleton_base_long.txt\",\n",
    "    \"test_mu_skeleton_base_medium.txt\",\n",
    "    \"test_mu_skeleton_change_empty.txt\",\n",
    "    \"test_mu_skeleton_change_long.txt\",\n",
    "    \"test_mu_skeleton_change_medium.txt\",\n",
    "    \"test_mu_skeleton_change+logging_long.txt\",\n",
    "    \"test_mu_skeleton_change+logging_medium.txt\",\n",
    "    \"test_novelWriter_skeleton_base_empty.txt\",\n",
    "    \"test_novelWriter_skeleton_base_long.txt\",\n",
    "    \"test_novelWriter_skeleton_base_medium.txt\",\n",
    "    \"test_novelWriter_skeleton_change_empty.txt\",\n",
    "    \"test_novelWriter_skeleton_change_long.txt\",\n",
    "    \"test_novelWriter_skeleton_change_medium.txt\",\n",
    "    \"test_novelWriter_skeleton_change+logging_long.txt\",\n",
    "    \"test_novelWriter_skeleton_change+logging_medium.txt\"\n",
    "]\n",
    "for file in file_names:\n",
    "    parsed = parse_file(file)\n",
    "    test_name = os.path.basename(file).replace(\".txt\",\"\")\n",
    "    for idx, row in enumerate(parsed,1):\n",
    "        row[\"test\"] = test_name\n",
    "        row[\"run\"] = idx\n",
    "        all_rows.append(row)\n",
    "df = pd.DataFrame(all_rows)\n",
    "\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19d43fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['energy_pkg', 'energy_cores', 'cycles', 'instructions',\n",
      "       'cache_references', 'cache_misses', 'cs', 'migrations', 'page_faults',\n",
      "       'test', 'run'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674a81e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               test      stat         p\n",
      "0                      test_leo_skeleton_base_empty  0.911920  0.016640\n",
      "1                       test_leo_skeleton_base_long  0.874439  0.002106\n",
      "2                     test_leo_skeleton_base_medium  0.922406  0.030980\n",
      "3             test_leo_skeleton_change+logging_long  0.896235  0.006802\n",
      "4           test_leo_skeleton_change+logging_medium  0.946061  0.132495\n",
      "5                    test_leo_skeleton_change_empty  0.907956  0.013220\n",
      "6                     test_leo_skeleton_change_long  0.874663  0.002131\n",
      "7                   test_leo_skeleton_change_medium  0.945490  0.127884\n",
      "8                       test_mu_skeleton_base_empty  0.971231  0.573386\n",
      "9                        test_mu_skeleton_base_long  0.947625  0.145974\n",
      "10                     test_mu_skeleton_base_medium  0.975716  0.703814\n",
      "11             test_mu_skeleton_change+logging_long  0.962999  0.368681\n",
      "12           test_mu_skeleton_change+logging_medium  0.954370  0.221011\n",
      "13                    test_mu_skeleton_change_empty  0.907068  0.012560\n",
      "14                     test_mu_skeleton_change_long  0.871720  0.001830\n",
      "15                   test_mu_skeleton_change_medium  0.865072  0.001304\n",
      "16             test_novelWriter_skeleton_base_empty  0.931163  0.052726\n",
      "17              test_novelWriter_skeleton_base_long  0.953552  0.210257\n",
      "18            test_novelWriter_skeleton_base_medium  0.937217  0.076556\n",
      "19    test_novelWriter_skeleton_change+logging_long  0.964469  0.400706\n",
      "20  test_novelWriter_skeleton_change+logging_medium  0.885401  0.003760\n",
      "21           test_novelWriter_skeleton_change_empty  0.959321  0.297601\n",
      "22            test_novelWriter_skeleton_change_long  0.962012  0.348359\n",
      "23          test_novelWriter_skeleton_change_medium  0.951079  0.180675\n",
      "Test cases that are normally distributed (p > 0.05):\n",
      "- test_leo_skeleton_change+logging_medium\n",
      "- test_leo_skeleton_change_medium\n",
      "- test_mu_skeleton_base_empty\n",
      "- test_mu_skeleton_base_long\n",
      "- test_mu_skeleton_base_medium\n",
      "- test_mu_skeleton_change+logging_long\n",
      "- test_mu_skeleton_change+logging_medium\n",
      "- test_novelWriter_skeleton_base_empty\n",
      "- test_novelWriter_skeleton_base_long\n",
      "- test_novelWriter_skeleton_base_medium\n",
      "- test_novelWriter_skeleton_change+logging_long\n",
      "- test_novelWriter_skeleton_change_empty\n",
      "- test_novelWriter_skeleton_change_long\n",
      "- test_novelWriter_skeleton_change_medium\n"
     ]
    }
   ],
   "source": [
    "# Test for normality - Shapiro-Wilk test\n",
    "\n",
    "shapiro_results = []\n",
    "for name, grp in df.groupby(\"test\"):\n",
    "    stat, p = shapiro(grp[\"energy_pkg\"])\n",
    "    shapiro_results.append({\"test\": name, \"stat\": stat, \"p\": p})\n",
    "norm_df = pd.DataFrame(shapiro_results)\n",
    "\n",
    "# Stat = values closer to 1 indicate normality\n",
    "# p = probability of observing stat as extreme as you did \n",
    "# p <= 0.05 (most commonly), we can reject null and conclude this sample significantly deviates from normal\n",
    "# p > 0.05, we don't have evidence the data isn't normal\n",
    "\n",
    "print(norm_df)\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "norm_df['is_normal'] = norm_df['p'] > alpha\n",
    "\n",
    "normal_tests = norm_df[norm_df['is_normal']]['test'].tolist()\n",
    "print(\"Test cases that are normally distributed (p > 0.05):\")\n",
    "for t in normal_tests:\n",
    "    print(f\"- {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a0706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['base', 'change', 'change+logging']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "194547a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normality per scenario:\n",
      "mu : W= 0.954 p= 0.221 - normal\n",
      "leo : W= 0.946 p= 0.132 - normal\n",
      "nw : W= 0.885 p= 0.004 - non-normal\n",
      "\n",
      "Variance:\n",
      "Levene’s test: W= 2.374705517933333 , p= 0.09904774395896343 - equal variance\n",
      "\n",
      "Kruskal–Wallis: H= 17.951090422175053 , p= 0.0001264649692480154 - statistically significant difference\n",
      "\n",
      "Dunn-Bonferroni pairwise p-values:\n",
      "          leo        mu        nw\n",
      "leo  1.000000  0.000222  0.003127\n",
      "mu   0.000222  1.000000  1.000000\n",
      "nw   0.003127  1.000000  1.000000\n"
     ]
    }
   ],
   "source": [
    "# repeat this for the two comparison types\n",
    "# repeat for different measured things (e.g. instructions or other energy cores)\n",
    "\n",
    "# chatGPT was used as an assistant to find general implementations of the statistical tests, which were then implemented by myself below\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "#groups_in_app = {\n",
    "#    \"base\": df.loc[df[\"test\"]==\"test_leo_skeleton_base_long\",\"energy_pkg\"].values,\n",
    "#    \"change\": df.loc[df[\"test\"]==\"test_leo_skeleton_change_long\",\"energy_pkg\"].values,\n",
    "#    \"change+logging\": df.loc[df[\"test\"]==\"test_leo_skeleton_change+logging_long\",\"energy_pkg\"].values\n",
    "#}\n",
    "\n",
    "groups_in_app = {\n",
    "    \"mu\": df.loc[df[\"test\"]==\"test_mu_skeleton_change+logging_medium\",\"energy_pkg\"].values,\n",
    "    \"leo\": df.loc[df[\"test\"]==\"test_leo_skeleton_change+logging_medium\",\"energy_pkg\"].values,\n",
    "    \"nw\": df.loc[df[\"test\"]==\"test_novelWriter_skeleton_change+logging_medium\",\"energy_pkg\"].values\n",
    "}\n",
    "\n",
    "print(\"Normality per scenario:\")\n",
    "normality = []\n",
    "for v, data in groups_in_app.items():\n",
    "    W, p = shapiro(data)\n",
    "    if p>alpha:\n",
    "        print(v, \": W=\", round(W, 3), \"p=\", round(p, 3), \"- normal\")\n",
    "    else:\n",
    "        print(v, \": W=\", round(W, 3), \"p=\", round(p, 3), \"- non-normal\")\n",
    "\n",
    "print(\"\\nVariance:\")\n",
    "stat_levene, p_levene = levene(*groups_in_app.values())\n",
    "if p_levene>alpha:\n",
    "    print(\"Levene’s test: W=\", stat_levene, \", p=\", p_levene, \"- equal variance\")\n",
    "else:\n",
    "    print(\"Levene’s test: W=\", stat_levene, \", p=\", p_levene, \"- unequal variance\")\n",
    "\n",
    "all_data = np.concatenate(list(groups_in_app.values()))\n",
    "\n",
    "# chatGPT generated the line below\n",
    "all_labels = np.repeat(list(groups_in_app.keys()), [len(g) for g in groups_in_app.values()])\n",
    "\n",
    "# If all groups are normally distributed (shapiro-wilk), and has equal variances (levene)\n",
    "if all(shapiro(g)[1] > alpha for g in groups_in_app.values()) and p_levene > alpha:\n",
    "    # Data is parametric\n",
    "    # Then we can do one way ANOVA\n",
    "    F, p_main = f_oneway(groups_in_app['base'], groups_in_app['change'], groups_in_app['change+logging'])\n",
    "    if p_main>alpha:\n",
    "        print(\"\\nANOVA: F=\", F, \", p=\", p_main, \"- no statistically significant difference\")\n",
    "    else:\n",
    "        print(\"\\nANOVA: F=\", F, \", p=\", p_main, \"- statistically significant difference\")\n",
    "\n",
    "    # Post‑hoc test - Tukey HSD\n",
    "    tukey = pairwise_tukeyhsd(endog=all_data, groups=all_labels, alpha=alpha)\n",
    "    print(\"\\nTukey HSD results:\")\n",
    "    print(tukey)\n",
    "else:\n",
    "    # Data is non-parametric\n",
    "    # We can do Kruskal-Wallis\n",
    "    #H, p_main = kruskal(groups_in_app['base'], groups_in_app['change'], groups_in_app['change+logging'])\n",
    "    H, p_main = kruskal(groups_in_app['leo'], groups_in_app['mu'], groups_in_app['nw'])\n",
    "    if p_main>alpha:\n",
    "        print(\"\\nKruskal–Wallis: H=\", H, \", p=\", p_main, \"- no statistically significant difference\")\n",
    "    else:\n",
    "        print(\"\\nKruskal–Wallis: H=\", H, \", p=\", p_main, \"- statistically significant difference\")\n",
    "\n",
    "    # Post‑hoc test - Dunn-Bonferroni\n",
    "    df_ph = pd.DataFrame({\n",
    "        \"value\": all_data,\n",
    "        \"group\": all_labels\n",
    "    })\n",
    "\n",
    "    dunn_res = sp.posthoc_dunn(df_ph, val_col='value', group_col='group', p_adjust='bonferroni')\n",
    "\n",
    "    print(\"\\nDunn-Bonferroni pairwise p-values:\")\n",
    "    print(dunn_res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2e32f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
