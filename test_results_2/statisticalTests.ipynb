{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e97bafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import shapiro, levene, f_oneway, kruskal\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "import scikit_posthocs as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74ff100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to the parsefile.py, we parse the results files:\n",
    "\n",
    "def parse_file(file_path):\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    blocks = re.split(r\"Performance counter stats for 'system wide':\", content)\n",
    "    rows = []\n",
    "\n",
    "    def clean_float(s):\n",
    "        return float(s.replace('\\u202F', '').replace(' ', '').replace(',', '.'))\n",
    "\n",
    "    def clean_int(s):\n",
    "        return int(s.replace('\\u202F', '').replace(' ', ''))\n",
    "\n",
    "    for block in blocks[1:]:\n",
    "        pkg_match = re.search(r\"([\\d,]+) Joules power/energy-pkg/\", block)\n",
    "        core_match = re.search(r\"([\\d,]+) Joules power/energy-cores/\", block)\n",
    "        cycles_match = re.search(r\"([\\d\\s\\u202F]+) +cycles\", block)\n",
    "        instr_match = re.search(r\"([\\d\\s\\u202F]+) +instructions\", block)\n",
    "        cache_ref_match = re.search(r\"([\\d\\s\\u202F]+) +cache-references\", block)\n",
    "        cache_miss_match = re.search(r\"([\\d\\s\\u202F]+) +cache-misses\", block)\n",
    "        cs_match = re.search(r\"([\\d\\s\\u202F]+) +cs\", block)\n",
    "        migrations_match = re.search(r\"([\\d\\s\\u202F]+) +migrations\", block)\n",
    "        pf_match = re.search(r\"([\\d\\s\\u202F]+) +page-faults\", block)\n",
    "\n",
    "        data = {\n",
    "            \"energy_pkg\": clean_float(pkg_match.group(1)),\n",
    "            \"energy_cores\": clean_float(core_match.group(1)),\n",
    "            \"cycles\": clean_int(cycles_match.group(1)),\n",
    "            \"instructions\": clean_int(instr_match.group(1)),\n",
    "            \"cache_references\": clean_int(cache_ref_match.group(1)),\n",
    "            \"cache_misses\": clean_int(cache_miss_match.group(1)),\n",
    "            \"cs\": clean_int(cs_match.group(1)),\n",
    "            \"migrations\": clean_int(migrations_match.group(1)),\n",
    "            \"page_faults\": clean_int(pf_match.group(1)),\n",
    "        }\n",
    "        rows.append(data)\n",
    "\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70037f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   energy_pkg  energy_cores      cycles  instructions  cache_references  \\\n",
      "0       58.15          2.62  1205632822     420613620          47533145   \n",
      "1       45.54          2.41  1172328882     358697847          44816818   \n",
      "2       53.22          2.33  1145813786     363630064          44353035   \n",
      "3       56.20          2.78  1288981070     374030561          47821323   \n",
      "4       61.27          2.63  1296263687     541997216          47250695   \n",
      "5       62.07          2.42  1166862379     360993520          45594577   \n",
      "6       55.56          2.36  1164006373     352163462          44268202   \n",
      "7       58.49          2.41  1240766886     434766567          45593474   \n",
      "8       55.31          2.80  1303753001     554700304          46004282   \n",
      "9       56.28          2.50  1195266861     373586452          45852171   \n",
      "\n",
      "   cache_misses     cs  migrations  page_faults                          test  \\\n",
      "0      23117788  11333          66         1120  test_leo_skeleton_base_empty   \n",
      "1      22199808  11053          48          201  test_leo_skeleton_base_empty   \n",
      "2      22176610  10928          72          873  test_leo_skeleton_base_empty   \n",
      "3      23557731  11526          62          176  test_leo_skeleton_base_empty   \n",
      "4      22868569  11576          86         2318  test_leo_skeleton_base_empty   \n",
      "5      21963082  10948          57          185  test_leo_skeleton_base_empty   \n",
      "6      22181958  10971          74          178  test_leo_skeleton_base_empty   \n",
      "7      23304415  11356         103         1166  test_leo_skeleton_base_empty   \n",
      "8      23066042  11644          86         2941  test_leo_skeleton_base_empty   \n",
      "9      22854915  11073          76          834  test_leo_skeleton_base_empty   \n",
      "\n",
      "   run  \n",
      "0    1  \n",
      "1    2  \n",
      "2    3  \n",
      "3    4  \n",
      "4    5  \n",
      "5    6  \n",
      "6    7  \n",
      "7    8  \n",
      "8    9  \n",
      "9   10  \n"
     ]
    }
   ],
   "source": [
    "all_rows = []\n",
    "\n",
    "# Create dataframe \n",
    "\n",
    "file_names = [\n",
    "    \"test_leo_skeleton_base_empty.txt\",\n",
    "    \"test_leo_skeleton_base_long.txt\",\n",
    "    \"test_leo_skeleton_base_medium.txt\",\n",
    "    \"test_leo_skeleton_change_empty.txt\",\n",
    "    \"test_leo_skeleton_change_long.txt\", \n",
    "    \"test_leo_skeleton_change_medium.txt\", \n",
    "    \"test_leo_skeleton_change+logging_long.txt\",\n",
    "    \"test_leo_skeleton_change+logging_medium.txt\",\n",
    "    \"test_leo_skeleton_change+logging_empty.txt\",\n",
    "    \"test_mu_skeleton_base_empty.txt\",\n",
    "    \"test_mu_skeleton_base_long.txt\",\n",
    "    \"test_mu_skeleton_base_medium.txt\",\n",
    "    \"test_mu_skeleton_change_empty.txt\",\n",
    "    \"test_mu_skeleton_change_long.txt\",\n",
    "    \"test_mu_skeleton_change_medium.txt\",\n",
    "    \"test_mu_skeleton_change+logging_long.txt\",\n",
    "    \"test_mu_skeleton_change+logging_medium.txt\",\n",
    "    \"test_mu_skeleton_change+logging_empty.txt\",\n",
    "    \"test_novelWriter_skeleton_base_empty.txt\",\n",
    "    \"test_novelWriter_skeleton_base_long.txt\",\n",
    "    \"test_novelWriter_skeleton_base_medium.txt\",\n",
    "    \"test_novelWriter_skeleton_change_empty.txt\",\n",
    "    \"test_novelWriter_skeleton_change_long.txt\",\n",
    "    \"test_novelWriter_skeleton_change_medium.txt\",\n",
    "    \"test_novelWriter_skeleton_change+logging_long.txt\",\n",
    "    \"test_novelWriter_skeleton_change+logging_medium.txt\",\n",
    "    \"test_novelWriter_skeleton_change+logging_empty.txt\"\n",
    "]\n",
    "for file in file_names:\n",
    "    parsed = parse_file(file)\n",
    "    test_name = os.path.basename(file).replace(\".txt\",\"\")\n",
    "    for idx, row in enumerate(parsed,1):\n",
    "        row[\"test\"] = test_name\n",
    "        row[\"run\"] = idx\n",
    "        all_rows.append(row)\n",
    "df = pd.DataFrame(all_rows)\n",
    "\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19d43fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['energy_pkg', 'energy_cores', 'cycles', 'instructions',\n",
      "       'cache_references', 'cache_misses', 'cs', 'migrations', 'page_faults',\n",
      "       'test', 'run'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "674a81e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               test      stat         p\n",
      "0                      test_leo_skeleton_base_empty  0.979687  0.817173\n",
      "1                       test_leo_skeleton_base_long  0.960833  0.325288\n",
      "2                     test_leo_skeleton_base_medium  0.970981  0.566355\n",
      "3            test_leo_skeleton_change+logging_empty  0.952592  0.198269\n",
      "4             test_leo_skeleton_change+logging_long  0.955131  0.231478\n",
      "5           test_leo_skeleton_change+logging_medium  0.945234  0.125869\n",
      "6                    test_leo_skeleton_change_empty  0.909404  0.014375\n",
      "7                     test_leo_skeleton_change_long  0.954016  0.216294\n",
      "8                   test_leo_skeleton_change_medium  0.926824  0.040461\n",
      "9                       test_mu_skeleton_base_empty  0.898541  0.007737\n",
      "10                       test_mu_skeleton_base_long  0.948513  0.154226\n",
      "11                     test_mu_skeleton_base_medium  0.886869  0.004070\n",
      "12            test_mu_skeleton_change+logging_empty  0.955932  0.242972\n",
      "13             test_mu_skeleton_change+logging_long  0.962320  0.354603\n",
      "14           test_mu_skeleton_change+logging_medium  0.939709  0.089332\n",
      "15                    test_mu_skeleton_change_empty  0.961143  0.331221\n",
      "16                     test_mu_skeleton_change_long  0.951610  0.186682\n",
      "17                   test_mu_skeleton_change_medium  0.978356  0.780371\n",
      "18             test_novelWriter_skeleton_base_empty  0.987779  0.974718\n",
      "19              test_novelWriter_skeleton_base_long  0.978339  0.779863\n",
      "20            test_novelWriter_skeleton_base_medium  0.948514  0.154237\n",
      "21   test_novelWriter_skeleton_change+logging_empty  0.960878  0.326150\n",
      "22    test_novelWriter_skeleton_change+logging_long  0.971352  0.576793\n",
      "23  test_novelWriter_skeleton_change+logging_medium  0.948916  0.158112\n",
      "24           test_novelWriter_skeleton_change_empty  0.961709  0.342294\n",
      "25            test_novelWriter_skeleton_change_long  0.968113  0.488930\n",
      "26          test_novelWriter_skeleton_change_medium  0.923167  0.032431\n",
      "Test cases that are normally distributed (p > 0.05):\n",
      "- test_leo_skeleton_base_empty\n",
      "- test_leo_skeleton_base_long\n",
      "- test_leo_skeleton_base_medium\n",
      "- test_leo_skeleton_change+logging_empty\n",
      "- test_leo_skeleton_change+logging_long\n",
      "- test_leo_skeleton_change+logging_medium\n",
      "- test_leo_skeleton_change_long\n",
      "- test_mu_skeleton_base_long\n",
      "- test_mu_skeleton_change+logging_empty\n",
      "- test_mu_skeleton_change+logging_long\n",
      "- test_mu_skeleton_change+logging_medium\n",
      "- test_mu_skeleton_change_empty\n",
      "- test_mu_skeleton_change_long\n",
      "- test_mu_skeleton_change_medium\n",
      "- test_novelWriter_skeleton_base_empty\n",
      "- test_novelWriter_skeleton_base_long\n",
      "- test_novelWriter_skeleton_base_medium\n",
      "- test_novelWriter_skeleton_change+logging_empty\n",
      "- test_novelWriter_skeleton_change+logging_long\n",
      "- test_novelWriter_skeleton_change+logging_medium\n",
      "- test_novelWriter_skeleton_change_empty\n",
      "- test_novelWriter_skeleton_change_long\n"
     ]
    }
   ],
   "source": [
    "# Test for normality - Shapiro-Wilk test\n",
    "\n",
    "shapiro_results = []\n",
    "for name, grp in df.groupby(\"test\"):\n",
    "    stat, p = shapiro(grp[\"energy_pkg\"])\n",
    "    shapiro_results.append({\"test\": name, \"stat\": stat, \"p\": p})\n",
    "norm_df = pd.DataFrame(shapiro_results)\n",
    "\n",
    "# Stat = values closer to 1 indicate normality\n",
    "# p = probability of observing stat as extreme as you did \n",
    "# p <= 0.05 (most commonly), we can reject null and conclude this sample significantly deviates from normal\n",
    "# p > 0.05, we don't have evidence the data isn't normal\n",
    "\n",
    "print(norm_df)\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "norm_df['is_normal'] = norm_df['p'] > alpha\n",
    "\n",
    "normal_tests = norm_df[norm_df['is_normal']]['test'].tolist()\n",
    "print(\"Test cases that are normally distributed (p > 0.05):\")\n",
    "for t in normal_tests:\n",
    "    print(f\"- {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "313a0706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([51.62, 49.02, 61.08, 60.68, 51.12, 47.25, 72.09, 62.32, 55.05,\n",
       "       44.41, 50.64, 65.06, 48.27, 61.6 , 52.61, 59.55, 47.98, 54.04,\n",
       "       56.8 , 51.99, 57.7 , 63.92, 54.2 , 49.6 , 51.97, 60.44, 66.91,\n",
       "       53.78, 54.1 , 51.56])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df[\"test\"]==\"test_novelWriter_skeleton_change+logging_empty\",\"energy_pkg\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "194547a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normality per scenario:\n",
      "base : W= 0.887 p= 0.004 - non-normal\n",
      "change : W= 0.978 p= 0.78 - normal\n",
      "change+logging : W= 0.94 p= 0.089 - normal\n",
      "\n",
      "Variance:\n",
      "Levene’s test: W= 2.110424534070289 , p= 0.12734845852854015 - equal variance\n",
      "\n",
      "Kruskal–Wallis: H= 6.475169295270684 , p= 0.039258604037308964 - statistically significant difference\n",
      "\n",
      "Dunn-Bonferroni pairwise p-values:\n",
      "                    base    change  change+logging\n",
      "base            1.000000  0.035678        0.334679\n",
      "change          0.035678  1.000000        1.000000\n",
      "change+logging  0.334679  1.000000        1.000000\n"
     ]
    }
   ],
   "source": [
    "# repeat this for the two comparison types\n",
    "# repeat for different measured things (e.g. instructions or other energy cores)\n",
    "\n",
    "# chatGPT was used as an assistant to find general implementations of the statistical tests, which were then implemented by myself below\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "groups_in_app = {\n",
    "    \"base\": df.loc[df[\"test\"]==\"test_mu_skeleton_base_medium\",\"energy_pkg\"].values,\n",
    "    \"change\": df.loc[df[\"test\"]==\"test_mu_skeleton_change_medium\",\"energy_pkg\"].values,\n",
    "    \"change+logging\": df.loc[df[\"test\"]==\"test_mu_skeleton_change+logging_medium\",\"energy_pkg\"].values\n",
    "}\n",
    "\n",
    "#groups_in_app = {\n",
    "#    \"mu\": df.loc[df[\"test\"]==\"test_mu_skeleton_change+logging_empty\",\"energy_pkg\"].values,\n",
    "#    \"leo\": df.loc[df[\"test\"]==\"test_leo_skeleton_change+logging_empty\",\"energy_pkg\"].values,\n",
    "#    \"nw\": df.loc[df[\"test\"]==\"test_novelWriter_skeleton_change+logging_empty\",\"energy_pkg\"].values\n",
    "#}\n",
    "\n",
    "print(\"Normality per scenario:\")\n",
    "normality = []\n",
    "for v, data in groups_in_app.items():\n",
    "    W, p = shapiro(data)\n",
    "    if p>alpha:\n",
    "        print(v, \": W=\", round(W, 3), \"p=\", round(p, 3), \"- normal\")\n",
    "    else:\n",
    "        print(v, \": W=\", round(W, 3), \"p=\", round(p, 3), \"- non-normal\")\n",
    "\n",
    "print(\"\\nVariance:\")\n",
    "stat_levene, p_levene = levene(*groups_in_app.values())\n",
    "if p_levene>alpha:\n",
    "    print(\"Levene’s test: W=\", stat_levene, \", p=\", p_levene, \"- equal variance\")\n",
    "else:\n",
    "    print(\"Levene’s test: W=\", stat_levene, \", p=\", p_levene, \"- unequal variance\")\n",
    "\n",
    "all_data = np.concatenate(list(groups_in_app.values()))\n",
    "\n",
    "# chatGPT generated the line below\n",
    "all_labels = np.repeat(list(groups_in_app.keys()), [len(g) for g in groups_in_app.values()])\n",
    "\n",
    "# If all groups are normally distributed (shapiro-wilk), and has equal variances (levene)\n",
    "if all(shapiro(g)[1] > alpha for g in groups_in_app.values()) and p_levene > alpha:\n",
    "    # Data is parametric\n",
    "    # Then we can do one way ANOVA\n",
    "    #F, p_main = f_oneway(groups_in_app['base'], groups_in_app['change'], groups_in_app['change+logging'])\n",
    "    F, p_main = f_oneway(groups_in_app['leo'], groups_in_app['mu'], groups_in_app['nw'])\n",
    "    if p_main>alpha:\n",
    "        print(\"\\nANOVA: F=\", F, \", p=\", p_main, \"- no statistically significant difference\")\n",
    "    else:\n",
    "        print(\"\\nANOVA: F=\", F, \", p=\", p_main, \"- statistically significant difference\")\n",
    "\n",
    "    # Post‑hoc test - Tukey HSD\n",
    "    tukey = pairwise_tukeyhsd(endog=all_data, groups=all_labels, alpha=alpha)\n",
    "    print(\"\\nTukey HSD results:\")\n",
    "    print(tukey)\n",
    "else:\n",
    "    # Data is non-parametric\n",
    "    # We can do Kruskal-Wallis\n",
    "    H, p_main = kruskal(groups_in_app['base'], groups_in_app['change'], groups_in_app['change+logging'])\n",
    "    #H, p_main = kruskal(groups_in_app['leo'], groups_in_app['mu'], groups_in_app['nw'])\n",
    "    if p_main>alpha:\n",
    "        print(\"\\nKruskal–Wallis: H=\", H, \", p=\", p_main, \"- no statistically significant difference\")\n",
    "    else:\n",
    "        print(\"\\nKruskal–Wallis: H=\", H, \", p=\", p_main, \"- statistically significant difference\")\n",
    "\n",
    "    # Post‑hoc test - Dunn-Bonferroni\n",
    "    df_ph = pd.DataFrame({\n",
    "        \"value\": all_data,\n",
    "        \"group\": all_labels\n",
    "    })\n",
    "\n",
    "    dunn_res = sp.posthoc_dunn(df_ph, val_col='value', group_col='group', p_adjust='bonferroni')\n",
    "\n",
    "    print(\"\\nDunn-Bonferroni pairwise p-values:\")\n",
    "    print(dunn_res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2e32f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
